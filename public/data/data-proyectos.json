{
    "contenidos_esp": [
      {
        "id": 1,
        "imgEncabezado": "../assets/img/bannerme_5.jpeg",
        "titulo": "Ejemplo de prueba JSON 1",
        "subtitulo": "Análisis Riguroso de la Influencia Publicitaria en Decisiones de Compra: Datos, Pruebas Estadísticas y Resultados.",
        "vermaslink": {"link": "#seccion1",
                        "texto": "Ver Más",
                        "icono": "arrow_downward",
                        "clase": "btn-large waves-effect waves-light teal lighten-1"
                        },
        "textParacaidas": "PROYECTO DE BUSINESS INTELLIGENCE",
        "clase_cont_audio": "hide",
        "herramientas": "Jupyter Notebook - VS Code - Python - Streamlit - Pandas - Numpy - Matplotlib - Statsmodels.",
        "introd55": "Introducción",
        "detalle55": "Detalle Del Proyecto",
        "conclu55": "Conclusión",
        "introduccion": ["Este proyecto tuvo como finalidad desarrollar una aplicación en Streamlit que permita identificar si una campaña publicitaria puede ser exitosa, y en que medida ese éxito se puede atribuir a los avisos recibidos por los clientes.",
                         "La base de datos empleada (Fuente : Kaggle - Marketing A/B Testing) proviene de un experimento donde los participantes se dividieron en dos grupos:",
                         "La elección de realizar una prueba de hipótesis Z de proporciones se basa en la necesidad de comparar la proporción de personas que realizaron compras en ambos grupos. Es esencial determinar si la campaña publicitaria genera efectos significativos en la conversión de clientes."
                        ],
        "desarrollo": [
                        {
                            
                            "texto": "Este es el primer párrafo con una imagen al inicio.",
                            "img": "../assets/img/portafolio-img/prueba-ab-strmlit/img-5-tit.png"
                        },
                        {
                            "texto": "Este es el primer párrafo con una imagen al inicio.",
                            "img": ""
                        },
                        {
                            "texto": "Este es el primer párrafo con una imagen al inicio.",
                            "img": "../assets/img/portafolio-img/prueba-ab-strmlit/img-5-tit.png"
                        },
                        {
                            "texto": "Este es el primer párrafo con una imagen al inicio.",
                            "img": "../assets/img/portafolio-img/prueba-ab-strmlit/img-5-tit.png"
                        }
                    ],
        "conclusion": ["Este proyecto tuvo como finalidad desarrollar una aplicación en Streamlit que permita identificar si una campaña publicitaria puede ser exitosa, y en que medida ese éxito se puede atribuir a los avisos recibidos por los clientes.",
                        "La base de datos empleada (Fuente : Kaggle - Marketing A/B Testing) proviene de un experimento donde los participantes se dividieron en dos grupos:",
                        "La elección de realizar una prueba de hipótesis Z de proporciones se basa en la necesidad de comparar la proporción de personas que realizaron compras en ambos grupos. Es esencial determinar si la campaña publicitaria genera efectos significativos en la conversión de clientes."
                        ],
        "textoSeccionEnlaces": "A continuación se dejan los enlaces relacionados con el proyecto:",
        "linkGithub": "https://github.com/martinbat89/Proyecto-AB-Con-Streamlit",
        "linkEnlaces1": {
                        "link": "https://proyecto-ab-con-app.streamlit.app/",
                        "textoBtn": "Streamlit",
                        "tooltip": "Ver app en streamlit.",
                        "icono": "ads_click",
                        "clase": "waves-effect waves-light red darken-1 btn-large tooltipped center"
                        },
        "linkEnlaces2": {
                        "link": "https://proyecto-ab-con-app.streamlit.app/",
                        "textoBtn": "Streamlit",
                        "tooltip": "Ver app en streamlit.",
                        "icono": "ads_click",
                        "clase": "waves-effect waves-light red darken-1 btn-large tooltipped"
                        }
            
      },
      {
        "id": 7,
        "imgEncabezado": "../assets/img/bannerme_5.jpeg",
        "titulo": "Etapa I - Proyecto Final Curso Big Data - Data Analytics Codo a Codo 4.0",
        "subtitulo": "Etapa I del proyecto final para obtener el certificado de Big Data - Data Analytics del programa Codo a Codo 4.0",
        "vermaslink": {"link": "#seccion1",
                        "texto": "Ver Más",
                        "icono": "arrow_downward",
                        "clase": "btn-large waves-effect waves-light teal lighten-1"
                        },
        "textParacaidas": "PROYECTO DE DATA ANALYTICS",
        "clase_cont_audio": "hide",
        "herramientas": "SQL - SQLite - Deepnote",
        "introd55": "Introducción",
        "detalle55": "Detalle Del Proyecto",
        "conclu55": "Conclusión",
        "introduccion": ["En este proyecto de análisis de datos, se utilizó SQL para explorar un conjunto de datos de películas de Netflix, con el objetivo de comprender mejor la composición y las características clave de su catálogo. El análisis exploratorio de datos (EDA) es una etapa crítica en cualquier proyecto de ciencia de datos, ya que permite descubrir patrones, identificar anomalías y establecer hipótesis iniciales. A través de una serie de consultas SQL, se realizó una investigación detallada que proporcionó una visión profunda del contenido disponible en la plataforma."
                           ],
        "desarrollo": [
                        {
                            
                            "texto": "El análisis comenzó con un enfoque en la estructura general de la base de datos, utilizando consultas SQL para contar el número total de registros en las tablas clave. Por ejemplo, se realizó un recuento total de las filas en las tablas content y production, lo que permitió tener una idea clara del volumen de datos disponibles para el análisis.",
                            "img": "../assets/img/portafolio-img/etapa1-bigdata-cac/5_count_row_prod.png"
                        },
                        {
                            "texto": "A continuación, se exploró la distribución de idiomas en las producciones disponibles. Para ello, se ejecutaron dos consultas clave. La primera consulta agrupó y contó el número de producciones en varios idiomas, incluidos Portugués, Francés, Alemán, Italiano y todas las variantes del Español. Esta consulta ofreció una visión global de los idiomas predominantes en el catálogo. La segunda consulta, más restrictiva, se centró únicamente en las producciones que estaban exclusivamente en uno de los idiomas mencionados, excluyendo cualquier contenido multilingüe. Este análisis fue esencial para comprender la diversidad lingüística de las producciones de Netflix.",
                            "img": "../assets/img/portafolio-img/etapa1-bigdata-cac/19_consulta_H.png"
                        },
                        {
                            "texto": "La segunda consulta, más restrictiva, se centró únicamente en las producciones que estaban exclusivamente en uno de los idiomas mencionados, excluyendo cualquier contenido multilingüe. Este análisis fue esencial para comprender la diversidad lingüística de las producciones de Netflix.",
                            "img": "../assets/img/portafolio-img/etapa1-bigdata-cac/18_consulta_H.png"
                        },
                        {
                            "texto": "El siguiente paso fue examinar la clasificación y el tipo de contenido disponible. Se utilizó una consulta que agrupó el contenido según su tipo (por ejemplo, película o serie) y su clasificación (como G, PG, etc.), proporcionando un resumen del contenido disponible por categoría y su respectiva clasificación. Este análisis fue útil para entender cómo se distribuye el contenido en términos de su adecuación para diferentes audiencias.",
                            "img": "../assets/img/portafolio-img/etapa1-bigdata-cac/17_consulta_G.png"
                        },
                        {
                            "texto": "Además, se realizaron consultas específicas para obtener detalles sobre contenido clasificado bajo la categoría 'G' y para recuperar información detallada de un contenido específico basado en su id_content. Por ejemplo, se extrajo el título, país de origen, duración y categorías listadas para una pieza de contenido específica, lo que permitió un análisis más profundo de casos particulares dentro del catálogo.",
                            "img": "../assets/img/portafolio-img/etapa1-bigdata-cac/10_consulta_A.png"
                        },
                        {
                            "texto": "El presente desarrollo solo muestra algunos ejemplos de consultas realizadas en SQL. Al final de esta página se dejará el link al notebook detallado.",
                            "img": ""
                        }
                    ],
        "conclusion": ["El análisis exploratorio realizado proporcionó una base sólida para comprender la composición del catálogo de la plataforma. Desde la distribución de idiomas hasta la clasificación y tipos de contenido, cada consulta aportó información valiosa que ayudará en la toma de decisiones futuras. Este proceso no solo permitió una mayor comprensión de los datos disponibles, sino que también estableció un marco robusto para análisis más complejos y detallados en etapas posteriores del proyecto. La capacidad de SQL para manejar grandes volúmenes de datos y realizar consultas detalladas fue fundamental para el éxito de esta etapa del proyecto."
                        ],
        "textoSeccionEnlaces": "A continuación se deja el link al notebook alojado en Deepnote, con todo el trabajo detallado y explicado:",
        "linkGithub": "",
        "linkEnlaces1": {
                        "link": "https://deepnote.com/workspace/martin1989-123c1f53-f57d-4bbf-9fef-0309c25663e8/project/C23679-Martin-Batarev-TPintegrador-6c13ed99-475c-4c70-a284-0d5ac8195c31/notebook/2.%20Consigna%20%232-0a7866ec6b7c47e28a297ea88c997c44",
                        "textoBtn": "Deepnote",
                        "tooltip": "Ver notebook en Deepnote.",
                        "icono": "ads_click",
                        "clase": "waves-effect waves-light blue darken-1 btn-large tooltipped"
                        },
        "linkEnlaces2": {
                        "link": "",
                        "textoBtn": "Streamlit",
                        "tooltip": "Ver app en streamlit.",
                        "icono": "ads_click",
                        "clase": "waves-effect waves-light red darken-1 btn-large tooltipped"
                        },
        "txt-volver": "Volver al Inicio"
            
      },
      {
        "id": 8,
        "imgEncabezado": "../assets/img/bannerme_5.jpeg",
        "titulo": "Aplicación Web de Reservas de Camping",
        "subtitulo": "Automatización de reservas para campings y hostels con Google Apps Script",
        "vermaslink": {
          "link": "#cont-audio",
          "texto": "Ver Más",
          "icono": "arrow_downward",
          "clase": "btn-large waves-effect waves-light teal lighten-1"
        },
        "textParacaidas": "SISTEMAS DE GESTIÓN ACCESIBLES",
        "clase_cont_audio": "hide",
        "titulo_audio": "Ecuchá el audio",
        "src_audio": "../assets/audio/proy_9.wav",
        "herramientas": "Google Apps Script, HTML, CSS, Javascript, Google Sheets, Materialize CSS, Flatpickr",
        "introd55": "Introducción",
        "detalle55": "Detalle Del Proyecto",
        "conclu55": "Conclusión",
        "introduccion": [
          "Este proyecto presenta una solución de bajo costo para la gestión de reservas en campings y hostels, desarrollada utilizando Google Apps Script. La aplicación permite digitalizar el proceso de reservas, aprovechando las herramientas de Google Drive y Sheets como base de datos, lo que facilita su implementación y mantenimiento."
        ],
        "desarrollo": [
            {
                "texto": "La aplicación está organizada en diferentes vistas, comenzando por una pantalla de inicio de sesión sencilla que permite a los usuarios ingresar al sistema utilizando una contraseña personal. Esto garantiza un acceso seguro y controlado a las funcionalidades de gestión de reservas.",
                "img": "../assets/img/portafolio-img/camping-res/img_campingdet1.png"
            },
            {
                "texto": "La pantalla principal ofrece una tarjeta interactiva que facilita la selección de la fecha deseada. Con solo un clic, los usuarios pueden ver el estado actualizado de las reservas del día. Se puede optar por dos tipos de visualización: una vista de mapa para una representación gráfica del camping o una vista de tabla que muestra las reservas de manera estructurada.",
                "img": "../assets/img/portafolio-img/camping-res/img_campingdet2.png"
            },
            {
                "texto": "La vista de mapa proporciona un esquema interactivo de las parcelas del camping. Los usuarios pueden hacer clic en cada parcela para ver un modal que les permite realizar operaciones CRUD (Crear, Leer, Actualizar, Eliminar) sobre las reservas. Este esquema visual hace que la gestión de las parcelas sea rápida e intuitiva.",
                "img": "../assets/img/portafolio-img/camping-res/img_campingdet4.png"
            },
            {
                "texto": "La funcionalidad CRUD también está disponible en la vista de tabla, donde la información de las reservas se presenta de forma detallada y ordenada. Los usuarios pueden gestionar sus reservas directamente desde esta vista, lo que ofrece flexibilidad en la gestión de datos.",
                "img": "../assets/img/portafolio-img/camping-res/img_campingdet3.png"
            },
            {
                "texto": "La aplicación incluye un modal de ayuda interactivo que orienta a los usuarios sobre el significado de los iconos y colores en la vista de mapa y tabla. Por ejemplo, un ícono 'rojo con destello' indica una reserva que ha hecho check-in pero aún no ha realizado el check-out. Esto permite un seguimiento más eficiente del estado de las reservas.",
                "img": "../assets/img/portafolio-img/camping-res/img_campingdet6.png"
            },
            {
                "texto": "Además, existe una sección de configuración independiente donde se pueden personalizar las opciones del camping, adaptando la aplicación a las necesidades específicas de cada establecimiento.",
                "img": "../assets/img/portafolio-img/camping-res/img_campingdet7.png"
            },
            {
                "texto": "Toda la información se almacena en Google Sheets, lo que facilita el acceso a los datos en tiempo real. Además, ofrece la posibilidad de conectarse con otros servicios como Looker para generar visualizaciones de datos atractivas o BigQuery para análisis avanzados.",
                "img": ""
            }
        ],
        "conclusion": [
          "Esta aplicación es una herramienta poderosa para campings o hostels que buscan digitalizar su proceso de reservas sin incurrir en costos elevados. Con solo una cuenta de Google Drive, la aplicación se puede implementar y gestionar fácilmente, ofreciendo una solución intuitiva y eficiente para la administración de reservas.",
          "Gracias a su interfaz amigable y la integración con Google Apps, esta solución es especialmente útil para pequeñas empresas que desean optimizar sus operaciones con un sistema digital."
        ],
        "textoSeccionEnlaces": "",
        "linkGithub": {
          "link": "https://github.com/usuario/proyecto-reservas-camping",
          "textoBtn": "GitHub",
          "tooltip": "Ver código en GitHub",
          "icono": "code",
          "clase": "waves-effect waves-light btn-large tooltipped"
        },
        "linkEnlaces1": {
          "link": "https://example.com/demo-proyecto-reservas-camping",
          "textoBtn": "Demo",
          "tooltip": "Ver demo en vivo",
          "icono": "visibility",
          "clase": "waves-effect waves-light btn-large tooltipped"
        },
        "linkEnlaces2": {
          "link": "https://drive.google.com/",
          "textoBtn": "Documentación",
          "tooltip": "Ver documentación",
          "icono": "description",
          "clase": "waves-effect waves-light btn-large tooltipped"
        },
        "txt-volver": "Volver al Inicio"
      },
      {
        "id": 9,
        "imgEncabezado": "../assets/img/bannerme_5.jpeg",
        "titulo": "Etapa II - Proyecto Final Curso Big Data - Data Analytics Codo a Codo 4.0",
        "subtitulo": "Análisis de datos de películas de Netflix y su relación con los Premios Oscar, como parte del curso Big Data - Data Analytics de Codo a Codo 4.0 (2023).",
        "vermaslink": {
          "link": "#seccion1",
          "texto": "Ver Más",
          "icono": "arrow_downward",
          "clase": "btn-large waves-effect waves-light teal lighten-1"
        },
        "textParacaidas": "PROYECTO DE DATA ANALYTICS",
        "clase_cont_audio": "hide",
        "titulo_audio": "Ecuchá el audio",
        "src_audio": "",
        "herramientas": "Python, SQL, SQLite",
        "introd55": "Introducción",
        "detalle55": "Detalle Del Proyecto",
        "conclu55": "Conclusión",
        "introduccion": [
          "Este proyecto se enfoca en el análisis de datos relacionados con las películas disponibles en Netflix y su conexión con los Premios Oscar. El objetivo principal fue explorar la base de datos de Netflix y un conjunto de datos de los Oscar para obtener información sobre las películas nominadas y ganadoras, así como su relación con las características de las películas en el catálogo de Netflix."
        ],
        "desarrollo": [
          {
            "texto": "El primer paso fue crear una tabla llamada 'oscar' en la base de datos de Netflix. Esta tabla se diseñó para almacenar información relevante sobre los premios Oscar, incluyendo el título de la película, el año de la ceremonia, la categoría, el nombre del nominado o ganador, y si ganó o no el premio.  Se utilizó SQL para definir la estructura de la tabla, especificando los tipos de datos y restricciones para cada columna.",
            "img": "../assets/img/portafolio-img/etapa2-bigdata-cac/1_sql.png"
          },
          {
            "texto": "Una vez creada la tabla 'oscar', se importaron los datos desde un archivo CSV llamado 'oscar.csv'. Este archivo contenía información histórica sobre los premios Oscar. Se utilizó la librería 'csv' de Python en conjunto con SQLite para leer el archivo CSV e insertar los datos en la tabla 'oscar' de la base de datos. Se verificó la integridad de los datos importados asegurándose de que todas las filas del CSV se insertaran correctamente en la tabla.",
            "img": "../assets/img/portafolio-img/etapa2-bigdata-cac/2_sql.png"
          },
          {
            "texto": "Después de la importación de datos, se realizaron varias consultas SQL para analizar la información. Estas consultas se enfocaron en explorar diferentes aspectos de la relación entre las películas de Netflix y los Premios Oscar. Por ejemplo, se buscaron películas que estuvieran presentes tanto en la base de datos de Netflix como en el conjunto de datos de los Oscar, se filtraron películas por género, año de lanzamiento, y otras características relevantes. Se vuelca una imágen de una de las consultas ejecutadas:",
            "img": "../assets/img/portafolio-img/etapa2-bigdata-cac/3_sql.png"
          },
          {
            "texto": "Un desafío importante durante el proyecto fue la inconsistencia en los IDs de contenido (id_content) entre las tablas 'oscar', 'content' y 'productions'.  Esta inconsistencia dificultó la realización de algunas consultas que requerían unir información de estas tablas.  Se documentaron estos problemas y se propusieron posibles soluciones para futuras iteraciones del proyecto, como la limpieza y estandarización de los IDs de contenido. Se vuelca un caso de ejemplo de la mencionada inconsistencia:",
            "img": "../assets/img/portafolio-img/etapa2-bigdata-cac/4_sql.png"
          },
          {
            "texto": "Además de las consultas principales, se realizaron consultas adicionales como desafíos. Estas consultas exploraron aspectos más específicos de los datos, como la distribución de los tipos de contenido en Netflix, la búsqueda de películas con duraciones específicas, y la identificación de películas con altas puntuaciones en Metacritic que también ganaron premios Oscar.  Se vuelca una imágen de una de las consultas ejecutadas:",
            "img": "../assets/img/portafolio-img/etapa2-bigdata-cac/5_sql.png"
          }
        ],
        "conclusion": [
          "Este proyecto proporcionó una valiosa experiencia en el análisis de datos utilizando SQL y SQLite. Se logró crear una nueva tabla en una base de datos, importar datos desde un CSV, y realizar diversas consultas para explorar la relación entre las películas de Netflix y los Premios Oscar.  La documentación detallada del proceso, incluyendo las consultas SQL, las capturas de pantalla y la descripción de los desafíos encontrados, facilita la comprensión del proyecto y su reproducibilidad.",
          "A pesar de las inconsistencias encontradas en los IDs de contenido, se obtuvieron resultados significativos que demuestran el potencial del análisis de datos para obtener información valiosa sobre el catálogo de Netflix y su relación con la industria cinematográfica.  En futuras iteraciones del proyecto, se podrían abordar las inconsistencias de datos y ampliar el análisis para incluir otras variables y fuentes de información."
        ],
        "textoSeccionEnlaces": "A continuación se deja el link al notebook alojado en Deepnote con todo el trabajo detallado y explicado:",
        "linkGithub": "",
        "linkEnlaces1": {
          "link": "https://deepnote.com/workspace/Martin1989-123c1f53-f57d-4bbf-9fef-0309c25663e8/project/c23679-Martin-Batarev-TPintegrador-941f5dd6-134a-43a2-8f7a-6e8bcbbe15f8/notebook/3-Consigna-3-59ee16693dac476583db479d17d8cec3?utm_source=share-modal&utm_medium=product-shared-content&utm_campaign=notebook&utm_content=941f5dd6-134a-43a2-8f7a-6e8bcbbe15f8",
          "textoBtn": "Deepnote",
          "tooltip": "Ver notebook en Deepnote.",
          "icono": "ads_click",
          "clase": "waves-effect waves-light blue darken-1 btn-large tooltipped"
        },
        "txt-volver": "Volver al Inicio"
      },
      {
        "id": 10,
        "imgEncabezado": "../assets/img/bannerme_5.jpeg",
        "titulo": "Predicción de Crímenes en Buenos Aires",
        "subtitulo": "Proyecto de Ciencia de Datos para predecir la cantidad de crímenes por tipo, barrio y franja horaria en la Ciudad de Buenos Aires.",
        "vermaslink": {
          "link": "#seccion1",
          "texto": "Ver Más",
          "icono": "arrow_downward",
          "clase": "btn-large waves-effect waves-light teal lighten-1"
        },
        "textParacaidas": "PROYECTO DE CIENCIA DE DATOS",
        "clase_cont_audio": "row",
        "titulo_audio": "Escuchá el audio",
        "src_audio": "../assets/audio/proy_10_esp.wav",
        "herramientas": "Pandas - Pandas Profiling - Jupyter Notebooks - KNNImputer - scikit-learn - Fitter - Regresión con Árboles de Decisión - Regresión con Bosques Aleatorios (Random Forest Regressor) - Pipeline - StandardScaler - PCA - RandomizedSearchCV - Streamlit - GitHub - ChatGPT - Google Drive",
        "introd55": "Introducción",
        "detalle55": "Detalle Del Proyecto",
        "conclu55": "Conclusión",
        "introduccion": [
          "Este proyecto, desarrollado por el Equipo D del curso de Fundamentos de Ciencia de Datos, aborda el desafío de predecir la cantidad de crímenes por tipo en la Ciudad de Buenos Aires.  Se utilizaron datos históricos proporcionados, abarcando un período de varios años, para construir un modelo de machine learning capaz de estimar la ocurrencia de crímenes. El objetivo principal es proporcionar una herramienta que pueda ser útil para la toma de decisiones en áreas como la asignación de recursos de seguridad y la planificación urbana. El proyecto se desarrolló siguiendo una metodología iterativa, que incluyó la exploración y limpieza de datos, la ingeniería de características, la selección y evaluación de modelos, y finalmente, el despliegue de una aplicación web interactiva para visualizar las predicciones."
        ],
        "desarrollo": [
          {
            "texto": "La etapa inicial del proyecto se centró en la definición precisa del problema y la variable objetivo. La consigna original, que solicitaba predecir la 'cantidad de crímenes por tipo', generó cierta ambigüedad.  El equipo tuvo que refinar esta definición, transformando un dataset de crímenes individuales en uno adecuado para un modelo predictivo.  Finalmente, se estableció que la variable objetivo sería la cantidad de crímenes mensuales por barrio, tipo de crimen y franja horaria. Esta definición permitió alinear el proyecto con las necesidades de predicción y facilitar la construcción del modelo.",
            "img": "../assets/img/portafolio-img/cac-fs-ccdatos-2024/img1.png"
          },
          {
            "texto": "La exploración y el preprocesamiento de los datos (EDA) constituyeron una fase crucial. Se analizaron cinco archivos de datos, que luego se unieron en un único dataset con más de 600,000 registros.  Se emplearon herramientas como Pandas y Pandas Profiling para realizar un análisis estadístico exhaustivo, identificar valores faltantes y repetidos, y analizar la distribución de las variables.  Se prestó especial atención a la homogeneidad de los datasets antes de su unión.  Los valores faltantes en campos como barrio, latitud y longitud se eliminaron, mientras que los faltantes en la franja horaria se imputaron utilizando el algoritmo KNNImputer, dada su baja proporción y la importancia de la precisión en la ubicación.",
            "img": "../assets/img/portafolio-img/cac-fs-ccdatos-2024/img2.png"
          },
          {
            "texto": "Un aspecto clave del preprocesamiento fue el tratamiento de los datos correspondientes a los meses de la pandemia de COVID-19.  Se observaron patrones atípicos en la ocurrencia de crímenes durante este período, lo que llevó a la decisión de eliminar estos registros del dataset.  Se asumió que las condiciones externas, como las restricciones de movilidad, habían alterado significativamente los patrones de criminalidad, y que su inclusión podría sesgar el modelo.  Esta decisión se tomó buscando que el modelo se basara en condiciones más estables y representativas.",
            "img": "../assets/img/portafolio-img/cac-fs-ccdatos-2024/img3.png"
          },
          {
            "texto": "La ingeniería de características y la agrupación de datos fueron fundamentales para la creación de la variable respuesta.  Se exploraron diversas combinaciones de variables, como la ubicación (barrio), el tiempo (año, mes, día de la semana) y el tipo de crimen.  Se extrajeron características de la fecha, como el cuatrimestre y el semestre, pero muchas fueron descartadas debido a su alta correlación o baja utilidad predictiva.  La franja horaria se agrupó en cuatro categorías (madrugada, mañana, tarde y noche) para simplificar el análisis.  Los tipos de crímenes también se agruparon en tres categorías principales (violentos, no violentos y viales), que luego se codificaron numéricamente. Se utilizó la librería Fitter para analizar la distribución de la variable respuesta resultante, identificando una distribución cercana a la log-normal.",
            "img": "../assets/img/portafolio-img/cac-fs-ccdatos-2024/img4.png"
          },
          {
            "texto": "En la fase de modelado, se probaron varios modelos de regresión utilizando pipelines de scikit-learn.  Inicialmente, se consideró un modelo de regresión lineal, pero no arrojó resultados satisfactorios.  Finalmente, se seleccionaron dos modelos: Decision Tree Regressor y Random Forest Regressor.  Ambos modelos alcanzaron un R² de 0.86 después de optimizar los hiperparámetros y descartar la variable 'mes' del entrenamiento.  Se utilizó StandardScaler para estandarizar las variables y PCA para la selección de componentes principales.  El análisis de los residuos reveló una distribución sesgada, pero el gráfico de valores reales vs. predichos mostró una buena correspondencia, especialmente para valores reales más bajos. Se utilizó RandomizedSearchCV, de Scikit Learn, para la optimización de hiperparámetros.",
            "img": "../assets/img/portafolio-img/cac-fs-ccdatos-2024/img5.png"
          },
          {
            "texto": "Para facilitar la interacción con el modelo y la visualización de las predicciones, se desarrolló una aplicación web interactiva utilizando Streamlit.  La aplicación permite al usuario ajustar diversos parámetros, como el tamaño de la muestra y los hiperparámetros del modelo (específicamente para Random Forest).  La aplicación re-entrena el modelo con los parámetros seleccionados y muestra la exactitud general, la cantidad total de crímenes estimada para un mes, un mapa con zonas de mayor riesgo, gráficos de agrupamiento por franja horaria y tipo de crimen, y un detalle de las cantidades estimadas por barrio, tipo y franja. El despliegue con Streamlit permitió una rápida implementación y una interfaz amigable para el usuario.",
            "img": "../assets/img/portafolio-img/cac-fs-ccdatos-2024/img6.png"
          }
        ],
        "conclusion": [
          "El proyecto demuestra la aplicación efectiva de los fundamentos de ciencia de datos para abordar un problema real y complejo: la predicción de crímenes en la Ciudad de Buenos Aires.  A pesar de los desafíos iniciales en la definición del problema y el procesamiento de un dataset extenso y complejo, el equipo logró desarrollar un modelo predictivo funcional y una herramienta de visualización interactiva.  Se aplicaron conceptos aprendidos en el curso, como la exploración y limpieza de datos, la ingeniería de características, la selección y evaluación de modelos, y el despliegue de aplicaciones web.",
          "El proyecto también resalta la importancia de la iteración y la adaptación en el proceso de ciencia de datos.  El equipo tuvo que realizar múltiples ajustes en la definición del problema, el preprocesamiento de los datos y la selección de modelos para lograr resultados satisfactorios.  Las lecciones aprendidas y los resultados obtenidos sientan una base sólida para futuras investigaciones y mejoras en este ámbito, como la incorporación de variables socioeconómicas, la exploración de otros modelos de regresión o técnicas de series de tiempo, y la optimización del rendimiento de la aplicación Streamlit."
        ],
        "textoSeccionEnlaces": "A continuación, se presentan enlaces de interés relacionados con el proyecto:",
        "linkGithub": {
          "tooltip": "Repositorio Github",
          "link": "https://github.com/martinbat89/stcr_codo_codo"

        },
        "linkEnlaces1": {
          "link": "https://colab.research.google.com/drive/1ebbIeSjWsQo5TB7BF6zF9utDig7MiKLy?usp=sharing",
          "textoBtn": "Colab Notebook Final",
          "tooltip": "Ver el notebook completo en Google Colab.",
          "icono": "ads_click",
          "clase": "waves-effect waves-light blue darken-1 btn-large tooltipped"
        },
        "linkEnlaces2": {
          "link": "https://stcrcodocodo-ljp5ydfwxptp2j5gjaekra.streamlit.app/",
          "textoBtn": "Streamlit App",
          "tooltip": "Access to the streamlit model web app.",
          "icono": "ads_click",
          "clase": "waves-effect waves-light red darken-1 btn-large tooltipped"
        },
        "txt-volver": "Volver al Inicio"
      }
      
    ],
    "contenidos_eng": [
      {
        "id": 1,
        "imgEncabezado": "../../assets/img/bannerme_5.jpeg",
        "titulo": "Example 1",
        "subtitulo": "Análisis Riguroso de la Influencia Publicitaria en Decisiones de Compra: Datos, Pruebas Estadísticas y Resultados.",
        "vermaslink": {"link": "#seccion1",
                        "texto": "Read More",
                        "icono": "arrow_downward",
                        "clase": "btn-large waves-effect waves-light teal lighten-1"
                        },
        "textParacaidas": "PROYECTO DE BUSINESS INTELLIGENCE",
        "clase_cont_audio": "hide",
        "herramientas": "Jupyter Notebook - VS Code - Python - Streamlit - Pandas - Numpy - Matplotlib - Statsmodels.",
        "introd55": "Introduction",
        "detalle55": "Detailed Project",
        "conclu55": "Conclusion",
        "introduccion": ["Este proyecto tuvo como finalidad desarrollar una aplicación en Streamlit que permita identificar si una campaña publicitaria puede ser exitosa, y en que medida ese éxito se puede atribuir a los avisos recibidos por los clientes.",
                         "La base de datos empleada (Fuente : Kaggle - Marketing A/B Testing) proviene de un experimento donde los participantes se dividieron en dos grupos:",
                         "La elección de realizar una prueba de hipótesis Z de proporciones se basa en la necesidad de comparar la proporción de personas que realizaron compras en ambos grupos. Es esencial determinar si la campaña publicitaria genera efectos significativos en la conversión de clientes."
                        ],
        "desarrollo": [
                        {
                            
                            "texto": "Este es el primer párrafo con una imagen al inicio.",
                            "img": "../../assets/img/portafolio-img/prueba-ab-strmlit/img-5-tit.png"
                        },
                        {
                            "texto": "Este es el primer párrafo con una imagen al inicio.",
                            "img": ""
                        },
                        {
                            "texto": "Este es el primer párrafo con una imagen al inicio.",
                            "img": "../../assets/img/portafolio-img/prueba-ab-strmlit/img-5-tit.png"
                        },
                        {
                            "texto": "Este es el primer párrafo con una imagen al inicio.",
                            "img": "../../assets/img/portafolio-img/prueba-ab-strmlit/img-5-tit.png"
                        }
                    ],
        "conclusion": ["Este proyecto tuvo como finalidad desarrollar una aplicación en Streamlit que permita identificar si una campaña publicitaria puede ser exitosa, y en que medida ese éxito se puede atribuir a los avisos recibidos por los clientes.",
                        "La base de datos empleada (Fuente : Kaggle - Marketing A/B Testing) proviene de un experimento donde los participantes se dividieron en dos grupos:",
                        "La elección de realizar una prueba de hipótesis Z de proporciones se basa en la necesidad de comparar la proporción de personas que realizaron compras en ambos grupos. Es esencial determinar si la campaña publicitaria genera efectos significativos en la conversión de clientes."
                        ],
        "textoSeccionEnlaces": "A continuación se dejan los enlaces relacionados con el proyecto:",
        "linkGithub": "https://github.com/martinbat89/Proyecto-AB-Con-Streamlit",
        "linkEnlaces1": {
                        "link": "https://proyecto-ab-con-app.streamlit.app/",
                        "textoBtn": "Streamlit",
                        "tooltip": "Ver app en streamlit.",
                        "icono": "ads_click",
                        "clase": "waves-effect waves-light red darken-1 btn-large tooltipped"
                        },
        "linkEnlaces2": {
                        "link": "https://proyecto-ab-con-app.streamlit.app/",
                        "textoBtn": "Streamlit",
                        "tooltip": "Ver app en streamlit.",
                        "icono": "ads_click",
                        "clase": "waves-effect waves-light red darken-1 btn-large tooltipped"
                        }
            
      },
      {
        "id": 7,
        "imgEncabezado": "../assets/img/bannerme_5.jpeg",
        "titulo": "Stage I - Final Project Big Data Course - Data Analytics Codo a Codo 4.0",
        "subtitulo": "Stage I of the final project to obtain the Big Data - Data Analytics certificate from the Codo a Codo 4.0 program",
        "vermaslink": {
            "link": "#seccion1",
            "texto": "Read More",
            "icono": "arrow_downward",
            "clase": "btn-large waves-effect waves-light teal lighten-1"
        },
        "textParacaidas": "DATA ANALYTICS PROJECT",
        "clase_cont_audio": "hide",
        "herramientas": "SQL - SQLite - Deepnote",
        "introd55": "Introduction",
        "detalle55": "Detailed Project",
        "conclu55": "Conclusion",
        "introduccion": [
            "In this data analysis project, SQL was used to explore a Netflix movie dataset to better understand the composition and key characteristics of its catalog. Exploratory data analysis (EDA) is a critical stage in any data science project as it allows for the discovery of patterns, identification of anomalies, and the establishment of initial hypotheses. Through a series of SQL queries, a detailed investigation was conducted that provided deep insight into the content available on the platform."
        ],
        "desarrollo": [
            {
                "texto": "The analysis began with a focus on the general structure of the database, using SQL queries to count the total number of records in the key tables. For example, a total count of rows in the content and production tables was performed, giving a clear idea of the volume of data available for analysis.",
                "img": "..//assets/img/portafolio-img/etapa1-bigdata-cac/5_count_row_prod.png"
            },
            {
                "texto": "Next, the distribution of languages in the available productions was explored. Two key queries were executed for this. The first query grouped and counted the number of productions in various languages, including Portuguese, French, German, Italian, and all variants of Spanish. This query provided a global view of the predominant languages in the catalog. ",
                 "img": "../assets/img/portafolio-img/etapa1-bigdata-cac/19_consulta_H.png"
            },
            {
                "texto": "The second, more restrictive query focused solely on productions that were exclusively in one of the mentioned languages, excluding any multilingual content. This analysis was essential to understanding the linguistic diversity of Netflix productions.",
                "img": "../assets/img/portafolio-img/etapa1-bigdata-cac/18_consulta_H.png"
            },
            {
                "texto": "The next step was to examine the classification and type of content available. A query was used that grouped the content by its type (e.g., movie or series) and its rating (such as G, PG, etc.), providing a summary of the available content by category and its respective rating. This analysis was useful in understanding how content is distributed in terms of its suitability for different audiences.",
                "img": "../assets/img/portafolio-img/etapa1-bigdata-cac/17_consulta_G.png"
            },
            {
                "texto": "Additionally, specific queries were made to obtain details about content classified under the 'G' category and to retrieve detailed information about specific content based on its id_content. For example, the title, country of origin, duration, and listed categories for a specific piece of content were extracted, allowing for a deeper analysis of particular cases within the catalog.",
                "img": "../assets/img/portafolio-img/etapa1-bigdata-cac/10_consulta_A.png"
            },
            {
                "texto": "This development only shows some examples of SQL queries. A link to the detailed notebook will be provided at the end of this page.",
                "img": ""
            }
        ],
        "conclusion": [
            "The exploratory analysis provided a solid foundation for understanding the composition of the platform's catalog. From the distribution of languages to the classification and types of content, each query provided valuable information that will aid in future decision-making. This process not only allowed for a greater understanding of the available data but also established a robust framework for more complex and detailed analyses in later stages of the project. SQL's ability to handle large volumes of data and perform detailed queries was crucial to the success of this stage of the project."
        ],
        "textoSeccionEnlaces": "Below is the link to the notebook hosted on Deepnote, with all the work detailed and explained:",
        "linkGithub": "",
        "linkEnlaces1": {
            "link": "https://deepnote.com/workspace/martin1989-123c1f53-f57d-4bbf-9fef-0309c25663e8/project/C23679-Martin-Batarev-TPintegrador-6c13ed99-475c-4c70-a284-0d5ac8195c31/notebook/2.%20Consigna%20%232-0a7866ec6b7c47e28a297ea88c997c44",
            "textoBtn": "Deepnote",
            "tooltip": "View notebook on Deepnote.",
            "icono": "ads_click",
            "clase": "waves-effect waves-light blue darken-1 btn-large tooltipped"
        },
        "linkEnlaces2": {
            "link": "",
            "textoBtn": "Deepnote",
            "tooltip": "View app on Streamlit.",
            "icono": "ads_click",
            "clase": "waves-effect waves-light red darken-1 btn-large tooltipped"
        },
        "txt-volver": "Back to home"
    },
    {
        "id": 8,
        "imgEncabezado": "../assets/img/bannerme_5.jpeg",
        "titulo": "Camping Reservation Web Application",
        "subtitulo": "Automating Reservations for Campgrounds and Hostels with Google Apps Script",
        "vermaslink": {
            "link": "#seccion1",
            "texto": "See More",
            "icono": "arrow_downward",
            "clase": "btn-large waves-effect waves-light teal lighten-1"
        },
        "textParacaidas": "ACCESSIBLE MANAGEMENT SYSTEMS",
        "clase_cont_audio": "hide",
        "herramientas": "Google Apps Script, HTML, CSS, Javascript, Google Sheets, Materialize CSS, Flatpickr",
        "introd55": "Introduction",
        "detalle55": "Detailed Project",
        "conclu55": "Conclusion",
        "introduccion": [
            "This project presents a low-cost solution for managing reservations in campgrounds and hostels, developed using Google Apps Script. The application digitizes the reservation process by leveraging Google Drive and Sheets as a database, making it easy to implement and maintain."
        ],
        "desarrollo": [
            {
                "texto": "The application is organized into various views, starting with a simple login screen that allows users to securely access the system with a personal password. This ensures controlled and secure access to the reservation management features.",
                "img": "../assets/img/portafolio-img/camping-res/img_campingdet1.png"
            },
            {
                "texto": "The main screen offers an interactive card to easily select the desired date. With just one click, users can view the updated status of the day's reservations. Two display options are available: a map view for a graphical representation of the campground or a table view that shows structured booking information.",
                "img": "../assets/img/portafolio-img/camping-res/img_campingdet2.png"
            },
            {
                "texto": "The map view provides an interactive layout of the campground plots. Users can click on each plot to open a modal that allows them to perform CRUD (Create, Read, Update, Delete) operations on the bookings. This visual scheme makes managing the plots quick and intuitive.",
                "img": "../assets/img/portafolio-img/camping-res/img_campingdet4.png"
            },
            {
                "texto": "CRUD functionality is also available in the table view, where reservation information is presented in a detailed and organized manner. Users can manage their bookings directly from this view, offering flexibility in data management.",
                "img": "../assets/img/portafolio-img/camping-res/img_campingdet3.png"
            },
            {
                "texto": "The application includes an interactive help modal that guides users on the meaning of icons and colors in the map and table views. For example, a 'flashing red' icon indicates a reservation with check-in but without check-out. This helps users track the status of reservations more efficiently.",
                "img": "../assets/img/portafolio-img/camping-res/img_campingdet6.png"
            },
            {
                "texto": "Additionally, there is an independent settings section where users can customize the camping options, allowing the app to be tailored to the specific needs of each establishment.",
                "img": "../assets/img/portafolio-img/camping-res/img_campingdet7.png"
            },
            {
                "texto": "All information is stored in Google Sheets, allowing real-time access to data. It also offers the possibility of connecting to other services like Looker to create attractive data visualizations or BigQuery for advanced analysis.",
                "img": ""
            }
        ],
        "conclusion": [
            "This application is a powerful tool for campgrounds or hostels looking to digitize their reservation process without high costs. With just a Google Drive account, the app can be easily implemented and managed, offering an intuitive and efficient solution for reservation management.",
            "Thanks to its user-friendly interface and Google Apps integration, this solution is especially useful for small businesses seeking to optimize their operations with a digital system."
        ],
        "textoSeccionEnlaces": "",
        "linkGithub": {
            "link": "https://github.com/usuario/proyecto-reservas-camping",
            "textoBtn": "GitHub",
            "tooltip": "View code on GitHub",
            "icono": "code",
            "clase": "waves-effect waves-light btn-large tooltipped"
        },
        "linkEnlaces1": {
            "link": "https://example.com/demo-proyecto-reservas-camping",
            "textoBtn": "Demo",
            "tooltip": "View live demo",
            "icono": "visibility",
            "clase": "waves-effect waves-light btn-large tooltipped"
        },
        "linkEnlaces2": {
            "link": "https://drive.google.com/",
            "textoBtn": "Documentation",
            "tooltip": "View documentation",
            "icono": "description",
            "clase": "waves-effect waves-light btn-large tooltipped"
        },
        "txt-volver": "Back to Home"
    },
    {
        "id": 9,
        "imgEncabezado": "../assets/img/bannerme_5.jpeg",
        "titulo": "Stage II - Final Project Big Data Course - Data Analytics Codo a Codo 4.0",
        "subtitulo": "Data analysis of Netflix movies and their relationship with the Oscar Awards, as part of the Big Data - Data Analytics course at Codo a Codo 4.0 (2023).",
        "vermaslink": {
          "link": "#seccion1",
          "texto": "See More",
          "icono": "arrow_downward",
          "clase": "btn-large waves-effect waves-light teal lighten-1"
        },
        "textParacaidas": "DATA ANALYTICS PROJECT",
        "clase_cont_audio": "hide",
        "titulo_audio": "Ecuchá el audio",
        "src_audio": "",
        "herramientas": "Python, SQL, SQLite",
        "introd55": "Introduction",
        "detalle55": "Detailed Project",
        "conclu55": "Conclusion",
        "introduccion": [
          "This project focuses on the data analysis of movies available on Netflix and their connection to the Oscar Awards. The main objective was to explore the Netflix database and an Oscar dataset to gain insights into nominated and winning films, as well as their relationship with the characteristics of movies in the Netflix catalog."
        ],
        "desarrollo": [
          {
            "texto": "The first step was to create a table named 'oscar' in the Netflix database. This table was designed to store relevant information about the Oscar awards, including the movie title, ceremony year, category, nominee or winner name, and whether they won the award. SQL was used to define the table structure, specifying data types and constraints for each column.",
            "img": "../assets/img/portafolio-img/etapa2-bigdata-cac/1_sql.png"
          },
          {
            "texto": "Once the 'oscar' table was created, data was imported from a CSV file named 'oscar.csv'. This file contained historical information about the Oscar awards. Python's 'csv' library, along with SQLite, was used to read the CSV file and insert the data into the 'oscar' table in the database. The integrity of the imported data was verified by ensuring that all rows from the CSV were correctly inserted into the table.",
            "img": "../assets/img/portafolio-img/etapa2-bigdata-cac/2_sql.png"
          },
          {
            "texto": "After data importation, several SQL queries were executed to analyze the information. These queries focused on exploring different aspects of the relationship between Netflix movies and the Oscar Awards. For instance, movies present in both the Netflix database and the Oscar dataset were searched, and movies were filtered by genre, release year, and other relevant characteristics. An image of one of the executed queries is displayed:",
            "img": "../assets/img/portafolio-img/etapa2-bigdata-cac/3_sql.png"
          },
          {
            "texto": "A significant challenge during the project was the inconsistency in content IDs (id_content) between the 'oscar', 'content', and 'productions' tables. This inconsistency made it difficult to perform some queries that required joining information from these tables. These issues were documented, and potential solutions for future project iterations were proposed, such as cleaning and standardizing content IDs. An example of the mentioned inconsistency is presented:",
            "img": "../assets/img/portafolio-img/etapa2-bigdata-cac/4_sql.png"
          },
          {
            "texto": "In addition to the main queries, further queries were performed as challenges. These queries explored more specific aspects of the data, such as the distribution of content types on Netflix, searching for movies with specific durations, and identifying movies with high Metacritic scores that also won Oscar awards. An image of one of the executed queries is displayed:",
            "img": "../assets/img/portafolio-img/etapa2-bigdata-cac/5_sql.png"
          }
        ],
        "conclusion": [
          "This project provided valuable experience in data analysis using SQL and SQLite.  We successfully created a new table in a database, imported data from a CSV file, and performed various queries to explore the relationship between Netflix movies and the Oscar Awards. The detailed documentation of the process, including SQL queries, screenshots, and a description of the challenges encountered, facilitates understanding and reproducibility of the project.",
          "Despite the inconsistencies found in content IDs, significant results were obtained, demonstrating the potential of data analysis to gain valuable insights into the Netflix catalog and its relationship with the film industry. In future iterations of the project, data inconsistencies could be addressed, and the analysis could be expanded to include other variables and information sources."
        ],
        "textoSeccionEnlaces": "Below is the link to the Deepnote notebook with all the detailed work and explanations:",
        "linkGithub": "",
        "linkEnlaces1": {
          "link": "https://deepnote.com/workspace/Martin1989-123c1f53-f57d-4bbf-9fef-0309c25663e8/project/c23679-Martin-Batarev-TPintegrador-941f5dd6-134a-43a2-8f7a-6e8bcbbe15f8/notebook/3-Consigna-3-59ee16693dac476583db479d17d8cec3?utm_source=share-modal&utm_medium=product-shared-content&utm_campaign=notebook&utm_content=941f5dd6-134a-43a2-8f7a-6e8bcbbe15f8",
          "textoBtn": "Deepnote",
          "tooltip": "See notebook in Deepnote.",
          "icono": "ads_click",
          "clase": "waves-effect waves-light blue darken-1 btn-large tooltipped"
        },
        "txt-volver": "Back to Home"
      },
      {
        "id": 10,
        "imgEncabezado": "../assets/img/bannerme_5.jpeg",
        "titulo": "Crime Prediction in Buenos Aires",
        "subtitulo": "Data Science Project to predict the number of crimes by type, neighborhood, and time slot in the City of Buenos Aires.",
        "vermaslink": {
          "link": "#seccion1",
          "texto": "See More",
          "icono": "arrow_downward",
          "clase": "btn-large waves-effect waves-light teal lighten-1"
        },
        "textParacaidas": "DATA SCIENCE PROJECT",
        "clase_cont_audio": "row",
        "titulo_audio": "Listen to audio",
        "src_audio": "../assets/audio/proy_10_eng.wav",
        "herramientas": "Pandas - Pandas Profiling - Jupyter Notebooks - KNNImputer - scikit-learn - Fitter - Decision Tree Regression - Random Forest Regression - Pipeline - StandardScaler - PCA - RandomizedSearchCV - Streamlit - GitHub - ChatGPT - Google Drive",
        "introd55": "Introduction",
        "detalle55": "Project Details",
        "conclu55": "Conclusion",
        "introduccion": [
          "This project, developed by Team D of the Data Science Fundamentals course, addresses the challenge of predicting the number of crimes by type in the City of Buenos Aires. Historical data provided, spanning several years, was used to build a machine learning model capable of estimating crime occurrence. The main objective is to provide a tool that can be useful for decision-making in areas such as security resource allocation and urban planning. The project was developed following an iterative methodology, which included data exploration and cleaning, feature engineering, model selection and evaluation, and finally, the deployment of an interactive web application to visualize the predictions."
        ],
        "desarrollo": [
          {
            "texto": "The initial stage of the project focused on the precise definition of the problem and the target variable. The original assignment, which requested to predict the 'number of crimes by type', generated some ambiguity. The team had to refine this definition, transforming a dataset of individual crimes into one suitable for a predictive model. Finally, it was established that the target variable would be the number of monthly crimes by neighborhood, type of crime, and time slot. This definition allowed the project to be aligned with prediction needs and facilitated the construction of the model.",
            "img": "../assets/img/portafolio-img/cac-fs-ccdatos-2024/img1.png"
          },
          {
            "texto": "Data exploration and preprocessing (EDA) constituted a crucial phase. Five data files were analyzed, which were then merged into a single dataset with more than 600,000 records. Tools such as Pandas and Pandas Profiling were used to perform a comprehensive statistical analysis, identify missing and repeated values, and analyze the distribution of variables. Special attention was paid to the homogeneity of the datasets before their union. Missing values in fields such as neighborhood, latitude, and longitude were removed, while missing values in the time slot were imputed using the KNNImputer algorithm, given their low proportion and the importance of location accuracy.",
            "img": "../assets/img/portafolio-img/cac-fs-ccdatos-2024/img2.png"
          },
          {
            "texto": "A key aspect of preprocessing was the handling of data corresponding to the months of the COVID-19 pandemic. Atypical patterns in crime occurrence were observed during this period, which led to the decision to remove these records from the dataset. It was assumed that external conditions, such as mobility restrictions, had significantly altered crime patterns, and that their inclusion could bias the model. This decision was made seeking that the model was based on more stable and representative conditions.",
            "img": "../assets/img/portafolio-img/cac-fs-ccdatos-2024/img3.png"
          },
          {
            "texto": "Feature engineering and data grouping were fundamental for the creation of the response variable. Various combinations of variables were explored, such as location (neighborhood), time (year, month, day of the week), and type of crime. Features were extracted from the date, such as the quarter and semester, but many were discarded due to their high correlation or low predictive utility. The time slot was grouped into four categories (early morning, morning, afternoon, and night) to simplify the analysis. Crime types were also grouped into three main categories (violent, non-violent, and traffic-related), which were then numerically encoded. The Fitter library was used to analyze the distribution of the resulting response variable, identifying a distribution close to log-normal.",
            "img": "../assets/img/portafolio-img/cac-fs-ccdatos-2024/img4.png"
          },
          {
            "texto": "In the modeling phase, several regression models were tested using scikit-learn pipelines. Initially, a linear regression model was considered, but it did not yield satisfactory results. Finally, two models were selected: Decision Tree Regressor and Random Forest Regressor. Both models achieved an R² of 0.86 after optimizing the hyperparameters and discarding the 'month' variable from the training. StandardScaler was used to standardize the variables and PCA for the selection of principal components. The analysis of the residuals revealed a skewed distribution, but the graph of actual vs. predicted values showed a good correspondence, especially for lower actual values. RandomizedSearchCV (Scikit Learn) was used for hyperparameter optimization.",
            "img": "../assets/img/portafolio-img/cac-fs-ccdatos-2024/img5.png"
          },
          {
            "texto": "To facilitate interaction with the model and visualization of predictions, an interactive web application was developed using Streamlit. The application allows the user to adjust various parameters, such as the sample size and model hyperparameters (specifically for Random Forest). The application retrains the model with the selected parameters and displays the overall accuracy, the total number of crimes estimated for a month, a map with higher-risk areas, graphs grouping by time slot and type of crime, and a detail of the estimated amounts by neighborhood, type, and time slot. Deployment with Streamlit allowed for rapid implementation and a user-friendly interface.",
            "img": "../assets/img/portafolio-img/cac-fs-ccdatos-2024/img6.png"
          }
        ],
        "conclusion": [
          "The project demonstrates the effective application of data science fundamentals to address a real and complex problem: crime prediction in the City of Buenos Aires. Despite the initial challenges in defining the problem and processing a large and complex dataset, the team managed to develop a functional predictive model and an interactive visualization tool. Concepts learned in the course were applied, such as data exploration and cleaning, feature engineering, model selection and evaluation, and web application deployment.",
          "The project also highlights the importance of iteration and adaptation in the data science process. The team had to make multiple adjustments to the problem definition, data preprocessing, and model selection to achieve satisfactory results. The lessons learned and the results obtained lay a solid foundation for future research and improvements in this area, such as the incorporation of socioeconomic variables, the exploration of other regression models or time series techniques, and the optimization of the performance of the Streamlit application."
        ],
        "textoSeccionEnlaces": "Below are links of interest related to the project:",
        "linkGithub": {
          "tooltip": "Github repository",
          "link": "https://github.com/martinbat89/stcr_codo_codo"

        },
        "linkEnlaces1": {
          "link": "https://colab.research.google.com/drive/1ebbIeSjWsQo5TB7BF6zF9utDig7MiKLy?usp=sharing",
          "textoBtn": "Colab Final Notebook",
          "tooltip": "View the complete notebook in Google Colab.",
          "icono": "ads_click",
          "clase": "waves-effect waves-light blue darken-1 btn-large tooltipped"
        },
        "linkEnlaces2": {
          "link": "https://stcrcodocodo-ljp5ydfwxptp2j5gjaekra.streamlit.app/",
          "textoBtn": "Streamlit App",
          "tooltip": "Access to the streamlit model web app.",
          "icono": "ads_click",
          "clase": "waves-effect waves-light red darken-1 btn-large tooltipped"
        },
        "txt-volver": "Back to Home"
      }
    
    
    ]

  }
  